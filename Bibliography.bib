Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Torre2015,
abstract = {Cancer constitutes an enormous burden on society in more and less economically developed countries alike. The occurrence of cancer is increasing because of the growth and aging of the population, as well as an increasing prevalence of established risk factors such as smoking, overweight, physical inactivity, and changing reproductive patterns associated with urbanization and economic development. Based on GLOBOCAN estimates, about 14.1 million new cancer cases and 8.2 million deaths occurred in 2012 worldwide. Over the years, the burden has shifted to less developed countries, which currently account for about 57{\%} of cases and 65{\%} of cancer deaths worldwide. Lung cancer is the leading cause of cancer death among males in both more and less developed countries, and has surpassed breast cancer as the leading cause of cancer death among females in more developed countries; breast cancer remains the leading cause of cancer death among females in less developed countries. Other leading causes of cancer death in more developed countries include colorectal cancer among males and females and prostate cancer among males. In less developed countries, liver and stomach cancer among males and cervical cancer among females are also leading causes of cancer death. Although incidence rates for all cancers combined are nearly twice as high in more developed than in less developed countries in both males and females, mortality rates are only 8{\%} to 15{\%} higher in more developed countries. This disparity reflects regional differences in the mix of cancers, which is affected by risk factors and detection practices, and/or the availability of treatment. Risk factors associated with the leading causes of cancer death include tobacco use (lung, colorectal, stomach, and liver cancer), overweight/obesity and physical inactivity (breast and colorectal cancer), and infection (liver, stomach, and cervical cancer). A substantial portion of cancer cases and deaths could be prevented by broadly applying effective prevention measures, such as tobacco control, vaccination, and the use of early detection tests.},
author = {Torre, Lindsey A. and Bray, Freddie and Siegel, Rebecca L. and Ferlay, Jacques and Lortet-Tieulent, Joannie and Jemal, Ahmedin},
doi = {10.3322/caac.21262},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/Torre{\_}et{\_}al-2015-CA{\_}{\_}A{\_}Cancer{\_}Journal{\_}for{\_}Clinicians.pdf:pdf},
issn = {1542-4863},
journal = {CA: A Cancer Journal for Clinicians},
keywords = {cancer,epidemiology,health disparities,incidence,survival},
number = {2},
pages = {87--108},
pmid = {25651787},
title = {{Global cancer statistics, 2012}},
volume = {65},
year = {2015}
}
@article{Heaton2018,
abstract = {This article presents results from an international collaboration between college students and pre-service teachers in Norway and the UK. This research is part of a large, international project exploring and developing the interrelationship between mobile technology and teachers' perceptions of teaching and learning. Data was collected for this study through an on-line survey of 37 pre-service teachers followed by six semi-structured, in-depth interviews. The data analysis revealed the themes of collaboration, authenticity and professional learning through the use of mobile technology in the data. The collaboration enabled the use of the affordances of mobile technology to enhance the pre-service teachers' professional learning and the data suggested that this enhanced their emergent conceptions of teaching and learning.},
author = {Heaton, Jeff},
doi = {10.1007/s10710-017-9314-z},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/Ian{\_}Goodfellow{\_}Yoshua{\_}Bengio{\_}and{\_}Aaron{\_}Courville{\_}D.pdf:pdf},
isbn = {0262035618},
issn = {1389-2576},
journal = {Genetic Programming and Evolvable Machines},
number = {1-2},
pages = {305--307},
publisher = {Springer US},
title = {{Ian Goodfellow, Yoshua Bengio, and Aaron Courville: Deep learning}},
volume = {19},
year = {2018}
}
@article{Gadermayr2019,
abstract = {A major challenge in the field of segmentation in digital pathology is given by the high effort for manual data annotations in combination with many sources introducing variability in the image domain. This requires methods that are able to cope with variability without requiring to annotate a large amount of samples for each characteristic. In this paper, we develop approaches based on adversarial models for image-to-image translation relying on unpaired training. Specifically, we propose approaches for stain-independent supervised segmentation relying on image-to-image translation for obtaining an intermediate representation. Furthermore, we develop a fully-unsupervised segmentation approach exploiting image-to-image translation to convert from the image to the label domain. Finally, both approaches are combined to obtain optimum performance in unsupervised segmentation independent of the characteristics of the underlying stain. Experiments on patches showing kidney histology proof that stain-translation can be performed highly effectively and can be used for domain adaptation to obtain independence of the underlying stain. It is even capable of facilitating the underlying segmentation task, thereby boosting the accuracy if an appropriate intermediate stain is selected. Combining domain adaptation with unsupervised segmentation finally showed the most significant improvements.},
author = {Gadermayr, Michael and Gupta, Laxmi and Appel, Vitus and Boor, Peter and Klinkhammer, Barbara M. and Merhof, Dorit},
doi = {10.1109/TMI.2019.2899364},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/08642295.pdf:pdf},
issn = {1558254X},
journal = {IEEE transactions on medical imaging},
number = {10},
pages = {2293--2302},
title = {{Generative Adversarial Networks for Facilitating Stain-Independent Supervised and Unsupervised Segmentation: A Study on Kidney Histology}},
volume = {38},
year = {2019}
}
@article{Wang2020,
abstract = {Automatic classification of breast histopathology images plays a key role in computer-aided breast cancer diagnosis. However, feature-based classification methods rely on the accurate cell segmentation and feature extraction. Due to overlapping cells, dust, impurities and uneven irradiation the accurate segmentation and efficient feature extraction are still challenging. In order to overcome the above difficulties and limited breast histopathology images, in this paper, a hybrid structure which includes a double deep transfer learning (D2TL) and interactive cross-task extreme learning machine (ICELM) is proposed based on feature extraction and representation ability of CNN and classification robustness of ELM. First, high level features are extracted using deep transfer learning and double-step deep transfer learning. Then, the high level feature sets are jointly used as regularization terms to further improve classification performance in interactive cross task extreme learning machine. The proposed method was tested on 134 breast cancer histopathology images. Results show that our method has achieved remarkable performance in classification accuracy (96.67{\%}, 96.96{\%}, 98.18{\%}). From the experiment result, the proposed method is promising for providing an efficient tool for breast cancer classification in clinical settings.},
author = {Wang, Pin and Song, Qi and Li, Yongming and Lv, Shanshan and Wang, Jiaxin and Li, Linyu and Zhang, He Hua},
doi = {10.1016/j.bspc.2019.101789},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/1-s2.0-S1746809419303702-main.pdf:pdf},
issn = {17468108},
journal = {Biomedical Signal Processing and Control},
keywords = {Breast cancer histopathology images,Convolutional neural networks,Double deep transfer learning,Interactive cross-task extreme learning machine,Uninvolved images},
pages = {101789},
publisher = {Elsevier Ltd},
title = {{Cross-task extreme learning machine for breast cancer image classification with deep convolutional features}},
url = {https://doi.org/10.1016/j.bspc.2019.101789},
volume = {57},
year = {2020}
}
@article{Dunnmon2019,
abstract = {Purpose: To assess the ability of convolutional neural networks (CNNs) to enable high-performance automated binary classification of chest radiographs. Materials and Methods: In a retrospective study, 216 431 frontal chest radiographs obtained between 1998 and 2012 were procured, along with associated text reports and a prospective label from the attending radiologist. This data set was used to train CNNs to classify chest radiographs as normal or abnormal before evaluation on a held-out set of 533 images hand-labeled by expert radiologists. The effects of development set size, training set size, initialization strategy, and network architecture on end performance were assessed by using standard binary classification metrics; detailed error analysis, including visualization of CNN activations, was also performed. Results: Average area under the receiver operating characteristic curve (AUC) was 0.96 for a CNN trained with 200 000 images. This AUC value was greater than that observed when the same model was trained with 2000 images (AUC = 0.84, P , .005) but was not significantly different from that observed when the model was trained with 20 000 images (AUC = 0.95, P . .05). Averaging the CNN output score with the binary prospective label yielded the best-performing classifier, with an AUC of 0.98 (P , .005). Analysis of specific radiographs revealed that the model was heavily influenced by clinically relevant spatial regions but did not reliably generalize beyond thoracic disease. Conclusion: CNNs trained with a modestly sized collection of prospectively labeled chest radiographs achieved high diagnostic performance in the classification of chest radiographs as normal or abnormal; this function may be useful for automated prioritization of abnormal chest radiographs.},
author = {Dunnmon, Jared A. and Yi, Darvin and Langlotz, Curtis P. and R{\'{e}}, Christopher and Rubin, Daniel L. and Lungren, Matthew P.},
doi = {10.1148/radiol.2018181422},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/radiol.2018181422.pdf:pdf},
issn = {15271315},
journal = {Radiology},
number = {3},
pages = {537--544},
title = {{Assessment of convolutional neural networks for automated classification of chest radiographs}},
volume = {290},
year = {2019}
}
@article{Christ2017,
author = {Christ, Patrick Ferdinand},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/1364397.pdf:pdf},
title = {{Convolutional Neural Networks for Classification and Segmentation of Medical Images}},
url = {http://mediatum.ub.tum.de/node?id=1364397},
year = {2017}
}
@article{Liu2019,
abstract = {Image classification aims to automatically group a set of images into several categorizations, which is widely applied in scene categorization, image clustering. Lung cancer recognition can be achieved by using image classification technique, since there are distinct differences between healthy lung and sick lung images. In this paper, we propose lung cancer recognition based on image quality assessment, which can distinguish sick lung images from healthy lung images. First, our dataset is acquired using low-dose CT scan combined with full-mode iterative recombination (IMR). Then, we incorporate both low-level and high-level features to extract deep representation from obtained dataset. Specifically, our designed low-level features include color moment and texture feature, and CNN based method is leveraged for deep feature extraction. For reducing artifacts and noise of images, we assign quality score for each training image. And quality score and deep feature are fused to generate deep representation. Afterward, we propose a probabilistic model to learn the distribution of deep representation. Finally, lung cancer recognition can be achieved using learned model. We conduct comprehensive experiments and our proposed method is verified effective.},
author = {Liu, Ying and Wang, Haodong and Gu, Yue and Lv, Xiaohong},
doi = {10.1016/j.jvcir.2019.06.012},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/1-s2.0-S1047320319301865-main.pdf:pdf},
issn = {10959076},
journal = {Journal of Visual Communication and Image Representation},
keywords = {CNN,Cancer recognition,Deep feature,Image classification},
pages = {102570},
publisher = {Elsevier Inc.},
title = {{Image classification toward lung cancer recognition by learning deep quality model}},
url = {https://doi.org/10.1016/j.jvcir.2019.06.012},
volume = {63},
year = {2019}
}
@article{Mohsen2018,
abstract = {Deep Learning is a new machine learning field that gained a lot of interest over the past few years. It was widely applied to several applications and proven to be a powerful machine learning tool for many of the complex problems. In this paper we used Deep Neural Network classifier which is one of the DL architectures for classifying a dataset of 66 brain MRIs into 4 classes e.g. normal, glioblastoma, sarcoma and metastatic bronchogenic carcinoma tumors. The classifier was combined with the discrete wavelet transform (DWT) the powerful feature extraction tool and principal components analysis (PCA) and the evaluation of the performance was quite good over all the performance measures.},
author = {Mohsen, Heba and El-Dahshan, El-Sayed A. and El-Horbaty, El-Sayed M. and Salem, Abdel-Badeeh M.},
doi = {10.1016/j.fcij.2017.12.001},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/1-s2.0-S2314728817300636-main.pdf:pdf},
issn = {23147288},
journal = {Future Computing and Informatics Journal},
keywords = {deep learning,deep neural network,discrete wavelet transform,fuzzy c-means,machine learning,magnetic resonance,principle component analysis},
number = {1},
pages = {68--71},
publisher = {Elsevier Ltd},
title = {{Classification using deep learning neural networks for brain tumors}},
url = {https://doi.org/10.1016/j.fcij.2017.12.001},
volume = {3},
year = {2018}
}
@article{Kallenberg2016,
abstract = {Mammographic risk scoring has commonly been automated by extracting a set of handcrafted features from mammograms, and relating the responses directly or indirectly to breast cancer risk. We present a method that learns a feature hierarchy from unlabeled data. When the learned features are used as the input to a simple classifier, two different tasks can be addressed: i) breast density segmentation, and ii) scoring of mammographic texture. The proposed model learns features at multiple scales. To control the models capacity a novel sparsity regularizer is introduced that incorporates both lifetime and population sparsity. We evaluated our method on three different clinical datasets. Our state-of-the-art results show that the learned breast density scores have a very strong positive relationship with manual ones, and that the learned texture scores are predictive of breast cancer. The model is easy to apply and generalizes to many other segmentation and scoring problems.},
author = {Kallenberg, Michiel and Petersen, Kersten and Nielsen, Mads and Ng, Andrew Y. and Diao, Pengfei and Igel, Christian and Vachon, Celine M. and Holland, Katharina and Winkel, Rikke Rass and Karssemeijer, Nico and Lillholm, Martin},
doi = {10.1109/TMI.2016.2532122},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/07412749.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Breast cancer,deep learning,mammograms,prognosis,risk factor,segmentation,unsupervised feature learning},
number = {5},
pages = {1322--1331},
publisher = {IEEE},
title = {{Unsupervised Deep Learning Applied to Breast Density Segmentation and Mammographic Risk Scoring}},
volume = {35},
year = {2016}
}
@article{Hu2018,
abstract = {In this paper, we aim to provide a survey on the applications of deep learning for cancer detection and diagnosis and hope to provide an overview of the progress in this field. In the survey, we firstly provide an overview on deep learning and the popular architectures used for cancer detection and diagnosis. Especially we present four popular deep learning architectures, including convolutional neural networks, fully convolutional networks, auto-encoders, and deep belief networks in the survey. Secondly, we provide a survey on the studies exploiting deep learning for cancer detection and diagnosis. The surveys in this part are organized based on the types of cancers. Thirdly, we provide a summary and comments on the recent work on the applications of deep learning to cancer detection and diagnosis and propose some future research directions.},
author = {Hu, Zilong and Tang, Jinshan and Wang, Ziming and Zhang, Kai and Zhang, Lin and Sun, Qingling},
doi = {10.1016/j.patcog.2018.05.014},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/1-s2.0-S0031320318301845-main.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
pages = {134--149},
publisher = {Elsevier Ltd},
title = {{Deep learning for image-based cancer detection and diagnosis − A survey}},
url = {https://doi.org/10.1016/j.patcog.2018.05.014},
volume = {83},
year = {2018}
}
@article{Tachibana2014,
author = {Tachibana, Hazuki},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/DeepLearningBook.pdf:pdf},
isbn = {9781491925614},
journal = {Sig2D'14},
pages = {1},
title = {{葉月ちゃんでもできる Deep Learning}},
url = {http://files.sig2d.org/sig2d14.pdf{\#}page=5},
year = {2014}
}
@book{Handels2009,
address = {Berlin Heidelberg New York},
author = {Handels, Heinz},
edition = {2. {\"{u}}berarb. u. erw. Aufl. 2009},
isbn = {978-3-835-10077-0},
publisher = {Springer-Verlag},
title = {{Medizinische Bildverarbeitung - Bildanalyse, Mustererkennung und Visualisierung f{\"{u}}r die computergest{\"{u}}tzte {\"{a}}rztliche Diagnostik und Therapie}},
year = {2009}
}
@article{Lundervold2019,
abstract = {What has happened in machine learning lately, and what does it mean for the future of medical image analysis? Machine learning has witnessed a tremendous amount of attention over the last few years. The current boom started around 2009 when so-called deep artificial neural networks began outperforming other established models on a number of important benchmarks. Deep neural networks are now the state-of-the-art machine learning models across a variety of areas, from image analysis to natural language processing, and widely deployed in academia and industry. These developments have a huge potential for medical imaging technology, medical data analysis, medical diagnostics and healthcare in general, slowly being realized. We provide a short overview of recent advances and some associated challenges in machine learning applied to medical image processing and image analysis. As this has become a very broad and fast expanding field we will not survey the entire landscape of applications, but put particular focus on deep learning in MRI. Our aim is threefold: (i)give a brief introduction to deep learning with pointers to core references; (ii)indicate how deep learning has been applied to the entire MRI processing chain, from acquisition to image retrieval, from segmentation to disease prediction; (iii)provide a starting point for people interested in experimenting and perhaps contributing to the field of deep learning for medical imaging by pointing out good educational resources, state-of-the-art open-source code, and interesting sources of data and problems related medical imaging.},
archivePrefix = {arXiv},
arxivId = {1811.10052},
author = {Lundervold, Alexander Selvikv{\aa}g and Lundervold, Arvid},
doi = {10.1016/j.zemedi.2018.11.002},
eprint = {1811.10052},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/1-s2.0-S0939388918301181-main.pdf:pdf},
issn = {18764436},
journal = {Zeitschrift fur Medizinische Physik},
keywords = {Deep learning,MRI,Machine learning,Medical imaging},
number = {2},
pages = {102--127},
publisher = {Elsevier B.V.},
title = {{An overview of deep learning in medical imaging focusing on MRI}},
url = {https://doi.org/10.1016/j.zemedi.2018.11.002},
volume = {29},
year = {2019}
}
@article{AbdelazizIsmael2020,
abstract = {Cancer is the second leading cause of death after cardiovascular diseases. Out of all types of cancer, brain cancer has the lowest survival rate. Brain tumors can have different types depending on their shape, texture, and location. Proper diagnosis of the tumor type enables the doctor to make the correct treatment choice and help save the patient's life. There is a high need in the Artificial Intelligence field for a Computer Assisted Diagnosis (CAD) system to assist doctors and radiologists with the diagnosis and classification of tumors. Over recent years, deep learning has shown an optimistic performance in computer vision systems. In this paper, we propose an enhanced approach for classifying brain tumor types using Residual Networks. We evaluate the proposed model on a benchmark dataset containing 3064 MRI images of 3 brain tumor types (Meningiomas, Gliomas, and Pituitary tumors). We have achieved the highest accuracy of 99{\%} outperforming the other previous work on the same dataset.},
author = {{Abdelaziz Ismael}, Sarah Ali and Mohammed, Ammar and Hefny, Hesham},
doi = {10.1016/j.artmed.2019.101779},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/1-s2.0-S0933365719306177-main.pdf:pdf},
issn = {18732860},
journal = {Artificial Intelligence in Medicine},
keywords = {Artificial neural network,Cancer classification,Convolutional neural network,Deep residual network,Machine learning},
number = {December 2019},
pages = {101779},
publisher = {Elsevier},
title = {{An enhanced deep learning approach for brain cancer MRI images classification using residual networks}},
url = {https://doi.org/10.1016/j.artmed.2019.101779},
volume = {102},
year = {2020}
}
@article{Alyafeai2020,
abstract = {Cervical cancer ranks the fourth most common cancer among females worldwide with roughly 528, 000 new cases yearly. Around 85{\%} of the new cases occurred in less-developed countries. In these countries, the high fatality rate is mainly attributed to the lack of skilled medical staff and appropriate medical pre-screening procedures. Images capturing the cervical region, known as cervigrams, are the gold-standard for the basic evaluation of cervical cancer presence. Cervigrams have high inter-rater variability especially among less skilled medical specialists. In this paper, we develop a fully-automated pipeline for cervix detection and cervical cancer classification from cervigram images. The proposed pipeline consists of two pre-trained deep learning models for the automatic cervix detection and cervical tumor classification. The first model detects the cervix region 1000 times faster than state-of-the-art data-driven models while achieving a detection accuracy of 0.68 in terms of intersection of union (IoU) measure. Self-extracted features are used by the second model to classify the cervix tumors. These features are learned using two lightweight models based on convolutional neural networks (CNN). The proposed deep learning classifier outperforms existing models in terms of classification accuracy and speed. Our classifier is characterized by an area under the curve (AUC) score of 0.82 while classifying each cervix region 20 times faster. Finally, the pipeline accuracy, speed and lightweight architecture make it very appropriate for mobile phone deployment. Such deployment is expected to drastically enhance the early detection of cervical cancer in less-developed countries.},
author = {Alyafeai, Zaid and Ghouti, Lahouari},
doi = {10.1016/j.eswa.2019.112951},
file = {:Users/dominiczillner/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/Studium/Master ITS/4. Semester/Masterarbeit Tumorerkennung/Theorie/1-s2.0-S0957417419306694-main.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Cervical cancer,Cervical region-of-interest,Cervix detection,Convolutional neural networks,Deep learning,Guanacaste and Intel{\&}mobileodt cervigram datasets},
publisher = {Elsevier Ltd},
title = {{A fully-automated deep learning pipeline for cervical cancer classification}},
volume = {141},
year = {2020}
}
@techreport{Eurostat2016,
abstract = {In allen Mitgliedstaaten war Lungenkrebs im Jahr 2013 die h{\"{a}}ufigste Krebsart mit Todesfolge. Der h{\"{o}}chste Anteil von Lungenkrebs an allen krebsbedingten Todesf{\"{a}}llen wurde in Ungarn (26{\%}) verzeichnet, gefolgt von Belgien, D{\"{a}}nemark, Griechenland, den Niederlanden (im Jahr 2012) und Polen (je 24{\%}), die niedrigsten Anteile wurde in Portugal (15{\%}) und Schweden (16{\%}) registriert. In fast jedem Mitgliedstaat waren von Lungenkrebs mehr M{\"{a}}nner als Frauen betroffen: auf EU-Ebene machte Lungenkrebs 26{\%} aller t{\"{o}}dlichen Krebserkrankungen bei M{\"{a}}nnern aus (185 600 Todesf{\"{a}}lle), gegen{\"{u}}ber 15{\%} bei Frauen (83 500 Todesf{\"{a}}lle). Dickdarmkrebs hatte den h{\"{o}}chsten Anteil an krebsbedingten Todesf{\"{a}}llen unter allen t{\"{o}}dlichen Krebsarten in Ungarn und der Slowakei (je 16{\%}) sowie in Spanien, Kroatien und Portugal (je 15{\%}). Dagegen entfielen auf Dickdarmkrebs weniger als 10{\%} aller krebsbedingten Todesf{\"{a}}lle in Griechenland und Zypern (je 9{\%}). Insgesamt waren M{\"{a}}nner und Frauen gleicherma{\ss}en von Dickdarmkrebs betroffen, in beiden Gruppen machte er 12{\%} aller t{\"{o}}dlichen Krebsarten aus. Das gleiche galt f{\"{u}}r Bauchspeicheldr{\"{u}}senkrebs, auf welchen 6{\%} aller krebsbedingten Todesf{\"{a}}lle in der m{\"{a}}nnlichen und 7{\%} in der weiblichen Bev{\"{o}}lkerung entfielen. Brustkrebs war im Jahr 2013 bei mehr als 92 000 Frauen in der EU todesurs{\"{a}}chlich Obwohl auch M{\"{a}}nner betroffen sein k{\"{o}}nnen, erkranken an Brustkrebs in erster Linie Frauen. Im Jahr 2013 stellte er f{\"{u}}r die weibliche Bev{\"{o}}lkerung in der EU mit 92 600 Todesf{\"{a}}llen (16{\%} aller krebsbedingten Todesf{\"{a}}lle bei Frauen) die h{\"{a}}ufigste t{\"{o}}dliche Krebsart dar. In den Mitgliedstaaten wurden die h{\"{o}}chsten Anteile von Todesf{\"{a}}llen aufgrund von Brustkrebs in der ausschlie{\ss}lich weiblichen Bev{\"{o}}lkerung in Zypern und Malta (je 21{\%} aller krebsbedingten Todesfalle bei Frauen) sowie in Luxemburg (20{\%}) und Belgien (19{\%}) registriert, w{\"{a}}hrend der niedrigste Anteile in Estland (12{\%}) verzeichnet wurde, gefolgt von der Tschechischen Republik, Polen und Schweden (je 14{\%}). Prostatakrebs, von dem Frauen nicht betroffen sind, war im Jahr 2013 die Ursache f{\"{u}}r 10{\%} aller krebsbedingten Todesf{\"{a}}lle bei M{\"{a}}nnern in der EU (72 700 Todesf{\"{a}}lle). Von den Mitgliedstaaten verzeichnete Schweden (20{\%} aller krebsbedingten Todesf{\"{a}}lle in der m{\"{a}}nnlichen Bev{\"{o}}lkerung) den h{\"{o}}chsten Anteil in der ausschlie{\ss}lich m{\"{a}}nnlichen Bev{\"{o}}lkerung, mit einigem Abstand gefolgt von D{\"{a}}nemark (15{\%}), Zypern und Finnland (je 14{\%}). Die niedrigsten Anteile wurden dagegen in Ungarn und Rum{\"{a}}nien (je 7{\%}) sowie in Italien, Luxemburg, Malta, Polen und der Slowakei (je 8{\%}) registriert. 26{\%} 12{\%} 6{\%} 10{\%} 46{\%} Lungenkrebs Dickdarmkrebs Bauchspeicheldr{\"{u}}senkrebs Prostatakrebs Andere Krebsarten Lungenkrebs ist die h{\"{a}}ufigste Krebsart mit Todesfolge bei M{\"{a}}nnern in der EU Frauen 15{\%} 12{\%} 7{\%} 16{\%} 50{\%} Lungenkrebs Dickdarmkrebs Bauchspeicheldr{\"{u}}senkrebs Brustkrebs Andere Krebsarten Brustkrebs ist die h{\"{a}}ufigste Krebsart mit Todesfolge bei Frauen in der EU},
author = {Eurostat},
file = {:Users/dominiczillner/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Lungenkrebs f{\"{u}}hrte im Jahr 2013 bei mehr als 185 000 M{\"{a}}nnern in der EU zum Tode Krebsarten mit Todesfolge in der.pdf:pdf},
isbn = {2243861220561},
title = {{Lungenkrebs f{\"{u}}hrte im Jahr 2013 bei mehr als 185 000 M{\"{a}}nnern in der EU zum Tode Krebsarten mit Todesfolge in der EU nach Krebsart und Geschlecht ({\%}), 2013 M{\"{a}}nner}},
year = {2016}
}
